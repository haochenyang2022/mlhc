{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BINF GU 4002: Machine Learning for Healthcare, Spring 2025\n",
    "# Assignment \\#1: Classification with Supervised Learning\n",
    "## DUE: 11:59 PM, Tuesday, February 18, 2025\n",
    "\n",
    "This assignment is a primer on binary classifiction with supervised learning. You will be implementing two classifiers and using them to train and evaluate models on a real world tabular dataset.\n",
    "\n",
    "**Instructions for accessing the dataset are below**--please make sure to read them. To save you some time, we have handled preprocessing the data for you.\n",
    "\n",
    "**<font color=\"red\">Please make sure that your Notebook runs (no running cells out of order!) and that your written answers are formatted using </font>$\\LaTeX$<font color=\"red\"> in `markdown` cells. When submitting, please name your files `{UNI}_binf4008_mlh_assignment_1.{filetype}` and submit both a `.ipynb` and `.html` version of your Jupyter notebook.</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Machine Learning Task\n",
    "\n",
    "You will be implementing two workhorse classification models--[Logistic Regression](https://en.wikipedia.org/wiki/Logistic_regression) and [Naïve Bayes](https://en.wikipedia.org/wiki/Naive_Bayes_classifier)--using base `numpy` in an [object oriented programmatic](https://en.wikipedia.org/wiki/Object-oriented_programming) way.\n",
    "\n",
    "In practice, training a classifier using [`LogisticRegression`](https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LogisticRegression.html) from `sklearn.linear_model` can be done with a handful of lines of code (same goes for [Naïve Bayes](https://scikit-learn.org/1.5/modules/naive_bayes.html)):\n",
    "\n",
    "```\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "predictions = lr.predict(X_train)\n",
    "```\n",
    "\n",
    "**The exercise here is to look under the proverbial \"hood\" of these two models.** We provide skeleton code and comments to help guide you.\n",
    "\n",
    "#### The Clinical Problem and Data\n",
    "\n",
    "We will be using the [Heart Disease](https://archive.ics.uci.edu/dataset/45/heart+disease) dataset to detect the presence of heart disease from tabular features. The dataset was used in \"[International application of a new probability algorithm for the diagnosis of coronary artery disease](https://www.sciencedirect.com/science/article/pii/0002914989905249)\" by Detrano et al. published in the *American Journal of Cardiology* in 1989 and sourced from the [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/). You can download the files directly from the UCI ML Repository or use the [`ucimlrepo`](https://github.com/uci-ml-repo/ucimlrepo) package.\n",
    "\n",
    "From the UCI Repository:\n",
    "\n",
    "> This database contains 76 attributes, but all published experiments refer to using a subset of 14 of them.  In particular, the Cleveland database is the only one that has been used by ML researchers to date.  The \"goal\" field refers to the ***presence of heart disease in the patient.***  It is integer valued from 0 (no presence) to 4. Experiments with the Cleveland database have concentrated on simply attempting to ***distinguish presence (values 1,2,3,4) from absence (value 0).***\n",
    "\n",
    "#### Evaluation\n",
    "\n",
    "Once you train your models, you have to generate performance metrics on the held-out test set. However, rather than generating a point estimate for the performance, **we are going to acquire confidence intervals by bootstrapping the test set** (see [Dr. Raschka's primer on this here](https://sebastianraschka.com/blog/2016/model-evaluation-selection-part2.html)). Not only is getting some sort of uncertainty estimate of our performance good science and reproducibility practice, it is becoming a requirement when disseminating your machine learning research. From the [AAAI Reproducibility Checklist](https://aaai.org/conference/aaai/aaai-23/reproducibility-checklist/):\n",
    "\n",
    "> Analysis of experiments goes beyond single-dimensional summaries of performance (e.g., average; median) to include measures of variation, confidence, or other distributional information. (yes/no)\n",
    "\n",
    "#### Best of Luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from ucimlrepo import fetch_ucirepo # uncomment if you are using the ucimlrepo package.\n",
    "from collections import Counter\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [45 Points] Question 1: Building the Naïve Bayes Classifier\n",
    "\n",
    "Our goal in Naïve Bayes is to get probabilities a feature vector $\\mathbf{x}=(x_1,\\ldots{},x_n)$ will belong to $K$ classes $C_k$ via conditional probabilities $P(C_k|\\mathbf{x})$. Questions 1.1 - 1.3 will be written (create a `markdown` cell) and 1.4 will be a code implementation.\n",
    "\n",
    "Starting with posterior $P(C_k|\\mathbf{x})$, prior $P(C_k)$, likelihood $P(\\mathbf{x}|C_k)$ and evidence $P(\\mathbf{x})$, and using the [Chain Rule of Probability](https://en.wikipedia.org/wiki/Chain_rule_(probability)) and the [Law of Total Probability](https://en.wikipedia.org/wiki/Law_of_total_probability), do the following:\n",
    "\n",
    "#### [5 Points] 1.1: Explain the Naïve Bayes assumption. Why is it \"naïve?\"\n",
    "\n",
    "Hint: You should discuss [conditional independence](https://en.wikipedia.org/wiki/Conditional_independence).\n",
    "\n",
    "Answer:\n",
    "\n",
    "We assume that all features are conditionally independent of each other given the class label.\n",
    "\n",
    "#### [10 Points] 1.2: Derive the expression below, starting with Bayes' Theorem:\n",
    "$$P(C_k|\\mathbf{x})=\\frac{1}{\\sum_{k}P(C_k)P(\\mathbf{x}|C_k)}P(C_k)\\prod^{n}_{i=1}P(x_i|C_k)$$\n",
    "\n",
    "Hints:\n",
    "- From Bayesian Probability: $\\text{posterior}=\\frac{\\text{prior}\\times{\\text{likelihood}}}{\\text{evidence}}$.\n",
    "- Think about what the denominator on the expression on the right means in context of the Law of Total Probability\n",
    "- $P(A,B)=P(B)P(A|B)$\n",
    "- $P(C_k,\\mathbf{x})=P(C_k,x_1, x_2,\\ldots{}, x_n)$\n",
    "- You need the Naïve Bayes Assumption.\n",
    "\n",
    "Answer:\n",
    "\n",
    "$$P(C_k|\\mathbf{x})=\\frac{P(C_k,\\mathbf{x})}{P(\\mathbf{x})}=\\frac{P(C_k,x_1, x_2,\\ldots{}, x_n)}{\\sum_{k}P(C_k)P(\\mathbf{x}|C_k)}=\\frac{P(C_k)P(x_1,x_2,\\ldots{},x_n|C_k)}{\\sum_{k}P(C_k)P(\\mathbf{x}|C_k)}=\\frac{P(C_k)\\prod^{n}_{i=1}P(x_i|C_k)}{\\sum_{k}P(C_k)P(\\mathbf{x}|C_k)}$$\n",
    "\n",
    "#### [10 Points] 1.3: We will implement a [Gaussian Naïve Bayes algorithm](https://scikit-learn.org/stable/modules/naive_bayes.html#gaussian-naive-bayes) using [Maximum Likelihood Estimation](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation) for our classification task. Starting from the expression you derviced in Question 1.2, explain how you will build your classifier by deriving the decision rule $\\text{argmax}_{y\\in\\{C_1,\\ldots{},C_k\\}}\\log(C_k|\\mathbf{x})$. \n",
    "\n",
    "Hints:\n",
    "- We are trying to select the class $C_k$ that maximizes the posterior probability $P(C_k|\\mathbf{x})$.\n",
    "- What are we assuming about the likelihood of the features? We are assuming that each feature follows a Gaussian distribution *class-dependent* mean and variance.\n",
    "    - What does this mean with respect to $P(x_i|C_k)$? Think about the Probability Density Function for the Gaussian distribution.\n",
    "- Since $\\frac{1}{\\sum_{k}P(C_k)P(\\mathbf{x}|C_k)}$ remains constant, you can ignore it during the maximization step.\n",
    "- A useful property of logarithms is that $\\log(\\prod_{i}P(x_i))=\\sum_{i}\\log(P(x_i))$. This is part of thr eason why we maximize the log-likelihood $\\log(P(C_k|\\mathbf{x}))$.\n",
    "\n",
    "Answer:\n",
    "\n",
    "We want to find $k$ that maximize $P(C_k|\\mathbf{x})=\\frac{P(C_k)\\prod^{n}_{i=1}P(x_i|C_k)}{\\sum_{k}P(C_k)P(\\mathbf{x}|C_k)}$. Since the denominator remains constant and log transfrom is monotonously increasing, all we need to maximize is $(\\log P(C_k)+\\sum^n_{i=1}\\log P(x_i|C_k))$.  $$k = \\text{argmax} P(C_k|\\mathbf{x}) = \\text{argmax} \\frac{P(C_k)\\prod^{n}_{i=1}P(x_i|C_k)}{\\sum_{k}P(C_k)P(\\mathbf{x}|C_k)} = \\text{argmax} \\bigg(\\log P(C_k)+\\sum^n_{i=1}\\log P(x_i|C_k)\\bigg) $$ \n",
    "\n",
    "We assume that each feature follows a Gaussian distribution class-dependent mean and variance. \n",
    "\n",
    "$$x_i|C_k \\sim N(\\mu_{ik},\\sigma_{ik}^2)$$ \n",
    "\n",
    "Suppose we have $m$ samples, each sample has $n$ features. The mean and variance can be estimated from data (maximum likelihood estimator of gaussian).\n",
    "\n",
    "$$ \\hat{\\mu_{k}} = \\frac{\\sum^m_{j=1}x_j\\cdot I(y_j = C_k)}{\\sum^m_{j=1}I(y_j = C_k)}$$ \n",
    "\n",
    "$$ \\hat{\\sigma^2_{k}} = \\frac{\\sum^m_{j=1}(x_j-\\hat{\\mu_{k}})^2\\cdot I(y_j = C_k)}{\\sum^m_{j=1}I(y_j = C_k)}$$ \n",
    "\n",
    "From MLE of class-dependent mean and variance, we can calculate $\\log P(x_i|C_k)$\n",
    "\n",
    "$$ \\log P(x_i|C_k) = \\log (\\frac{1}{\\sqrt{2\\pi\\hat{\\sigma^2_{ik}}}}\\exp(\\frac{(x_i-\\hat{\\mu_{ik}})}{2\\hat{\\sigma^2_{ik}}})) =  -\\frac{1}{2}\\log(2\\pi\\hat{\\sigma_{ik}^2})+ \\frac{(x_i-\\hat{\\mu_{ik}})^2}{2\\hat{\\sigma_{ik}^2}}$$\n",
    "\n",
    "\n",
    "The class prior $P(C_k)$ can be estimated directly from the label frequency.\n",
    "\n",
    "$$ \\log P(C_k) = \\log \\frac{\\sum^{m}_{i=1}I(y_i=C_k)}{m}$$\n",
    "\n",
    "\n",
    "Combine two log-likelihood,\n",
    "\n",
    "$$ k =  \\text{argmax} \\bigg(\\log P(C_k)+\\sum^n_{i=1}\\log P(x_i|C_k)\\bigg)$$\n",
    "\n",
    "$$ \\hat{y} = C_k $$\n",
    "\n",
    "So we first estimate the mean and variance for each class, then calculate the log prior and log likelihood. By summing them, we obtain the log posterior likelihood for each class. The class with the maximun posterior likelihood is chosen as the predicted class.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### [20 Points] 1.4: Implement a Naïve Bayes classifier with the skeleton code below.\n",
    "\n",
    "#### You are allowed to reference the following materials:\n",
    "- Lecture slides and notes from our course.\n",
    "- The Wikipedia articles on [Naïve Bayes](https://en.wikipedia.org/wiki/Naive_Bayes_classifier), the [Chain Rule of Probability](https://en.wikipedia.org/wiki/Chain_rule_(probability)) and the [Law of Total Probability](https://en.wikipedia.org/wiki/Law_of_total_probability).\n",
    "- Dr. Kilian Q. Weinberger's [course notes](https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote05.html) and lecture recording.\n",
    "- Dr. Michael Collin's [notes on Naïve Bayes and Expectation Maximization](http://www.cs.columbia.edu/~mcollins/em.pdf).\n",
    "- [Scikit-Learn documentation on Naïve Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    \n",
    "    def __init__(self):\n",
    "\n",
    "        self.class_probs = None        # P(C_k), the prior\n",
    "        self.class_means = None\n",
    "        self.class_variances = None\n",
    "        self.classes = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "\n",
    "        # Number of samples and features  \n",
    "        n_samples, n_features = X.shape\n",
    "        # Get unique class labels\n",
    "        self.classes = np.unique(y)\n",
    "        n_classes = len(self.classes)\n",
    "        \n",
    "        # Use np.zeros() to initialize the following:\n",
    "        # class_probs with shape n_classes\n",
    "        # class_means with shape (n_classes, n_features)\n",
    "        # class_variances with (n_classes, n_features)\n",
    "        self.class_probs = np.zeros(n_classes)\n",
    "        self.class_means = np.zeros((n_classes, n_features))\n",
    "        self.class_variances = np.zeros((n_classes, n_features))\n",
    "        \n",
    "        # X_c: index the features that belong to a class c (hint: you should use the value of y to index X)\n",
    "        # class_probs: number of rows in X_c / # of samples in X\n",
    "        # class_means is the mean of X_c along the 0th axis\n",
    "        # class_var is the variance of X_c aong the 0th axis\n",
    "        for i, c in enumerate(self.classes):\n",
    "\n",
    "            X_c = np.array([X[j] for j in range(n_samples) if y[j] == c])\n",
    "            self.class_probs[i] = X_c.shape[0] / n_samples\n",
    "            self.class_means[i, :] = np.mean(X_c, axis=0)\n",
    "            self.class_variances[i, :] = np.var(X_c, axis=0)\n",
    "    \n",
    "    def _calculate_log_likelihood(self, X):  \n",
    "\n",
    "        # get the number of samples. remember, X.shape = (n_samples, n_features). \n",
    "        n_samples = X.shape[0]\n",
    "        n_features = X.shape[1]\n",
    "        # instantiate the likelihood with an array of zeros of shape (n_samples, n_classes). You can do this using np.zeros()\n",
    "        likelihood = np.zeros((n_samples, len(self.classes)))\n",
    "        \n",
    "        # Iterate over the classes\n",
    "        for i, _ in enumerate(self.classes):\n",
    "\n",
    "            # get the mean and variance for the current class (by indexing class_means[i,:] and class_variances[i,:])\n",
    "            mean = self.class_means[i, :]\n",
    "            var = self.class_variances[i, :]\n",
    "            \n",
    "            # Calculate the Gaussian Likelihood and populate the [:,i]-th slice of likelihood array with the current class likelihood\n",
    "            likelihood[:, i] = np.sum(-0.5 * np.log(2 * np.pi * var) - 0.5 * ((X - mean) ** 2) / var, axis=1)\n",
    "        \n",
    "        return likelihood\n",
    "    \n",
    "    def _calculate_log_posterior(self, X) :\n",
    "\n",
    "        # log(posterior) = log(prior) + log(likelihood)\n",
    "        prior = np.log(self.class_probs)\n",
    "        likelihood = self._calculate_log_likelihood(X)\n",
    "        posterior = prior + likelihood\n",
    "\n",
    "        return posterior\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        log_posterior = self._calculate_log_posterior(X)\n",
    "        \n",
    "        return self.classes[np.argmax(log_posterior, axis=1)]\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "\n",
    "        log_posterior = self._calculate_log_posterior(X)\n",
    "        \n",
    "        posterior = np.exp(log_posterior)\n",
    "        probabilities = posterior / np.sum(posterior, axis=1, keepdims = True)\n",
    "        \n",
    "        return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [45 Points] Question 2: Building the Standard Logistic Regression Classifier\n",
    "\n",
    "We are going to build toward a Logistic Regression classifier. This is one of the most important machine learning models that is used widely in healthcare and other industries.\n",
    "\n",
    "Unlike the Naïve Bayes classifier, Logistic Regression has an optimization step. If you feel particularly lost in this question, start by reading Sections 5.1, 52, 5.4, and 5.6 in the [Logistic Regression chapter](https://web.stanford.edu/~jurafsky/slp3/5.pdf) in Dr. Jrafsky and Dr. Martin's *Speech and Language Processing*.\n",
    "\n",
    "#### [10 Points] Question 2.1: Derive the log-loss minimization $L(\\hat{y},y)$ for Logistic Regression. Start with the idea that we can model binary labels $P(y=1 \\vert \\mathbf{x}) \\sim \\text{Bernoulli}(\\sigma(\\mathbf{w}^T \\mathbf{x}))$, where $\\sigma{(\\mathbf{x})}=\\frac{1}{1+e^{-\\mathbf{x}}}$ is the sigmoid function.\n",
    "\n",
    "Hint: take the following steps.\n",
    "- Start with $\\hat{y}={\\sigma(\\mathbf{w}^T\\mathbf{x})}$\n",
    "- Bernoulli means that $P(y=1\\vert{{\\mathbf{x}}})=\\hat{y}^{y}(1-\\hat{y})^{1-y}$.\n",
    "- $L(\\hat{y},y)=-\\log{\\left(P(y=1\\vert{\\mathbf{x}})\\right)}$. Replace $\\hat{y}$ in terms of $\\mathbf{x}$ and expand the expression.\n",
    "\n",
    "Answer:\n",
    "\n",
    "$$L(\\hat{y},y)=-\\log{\\left(P(y=1\\vert{\\mathbf{x}})\\right)} = -\\log (\\hat{y}^{y}(1-\\hat{y})^{1-y}) = -\\big[y\\log\\hat{y}+(1-y)\\log(1-\\hat{y})\\big] = -\\big[y\\log\\sigma(\\mathbf{w}^T\\mathbf{x})+(1-y)\\log(1-\\sigma(\\mathbf{w}^T\\mathbf{x}))\\big]$$\n",
    "\n",
    "\n",
    "\n",
    "#### [10 Points] Question 2.2: Get the derivative of $\\sigma{(\\mathbf{w}^T \\mathbf{x})}$ with respect to $w_j\\in{}\\mathbf{w}$.\n",
    "\n",
    "Hint: Plug in $\\mathbf{w}^T \\mathbf{x}$ to $\\sigma{(\\mathbf{x})}=\\frac{1}{1+e^{-\\mathbf{x}}}$. Then, either use the quotient rule of partial derivatives:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial{x}}\\frac{f(x,y)}{g(x,y)}=\\frac{\\frac{\\partial{f(x,y)}}{\\partial{x}}g(x,y)-f(x,y)\\frac{\\partial{g(x,y)}}{\\partial{x}}}{\\left(g(x,y)\\right)^2}$$\n",
    "\n",
    "or express $\\sigma{(\\mathbf{x})}$ as $(1+e^{-\\mathbf{x}})^{-1}$ and use the chain rule.\n",
    "\n",
    "Answer:\n",
    "\n",
    "$$\\frac{\\partial\\sigma{(\\mathbf{w}^T \\mathbf{x})}}{\\partial{\\mathbf{w}}}=\\frac{\\partial\\sigma{(\\mathbf{w}^T \\mathbf{x})}}{\\partial(\\mathbf{w}^T\\mathbf{x})}\\frac{\\partial(\\mathbf{w}^T\\mathbf{x})}{\\partial{\\mathbf{w}}}=-\\frac{1}{(1+e^{-\\mathbf{w}^T \\mathbf{x}})^2}\\cdot e^{-\\mathbf{w}^T \\mathbf{x}} \\cdot \\mathbf{x}$$\n",
    "\n",
    "#### [10 Points] Question 2.3: Derive a gradient descent update to optimize for $\\mathbf{w}$.\n",
    "\n",
    "Hint: This is the derivative of the expression you derived from Question 2.1 ($L(\\hat{y},y)$) with respect to $w_j\\in{\\mathbf{w}}$.\n",
    "\n",
    "$$\\frac{\\partial L(\\hat{y},y)}{\\partial \\mathbf{w}} = -\\big[y\\cdot (1+e^{-\\mathbf{w}^T\\mathbf{x}})\\cdot -\\frac{1}{(1+e^{-\\mathbf{w}^T\\mathbf{x}})^2}\\cdot (e^{-\\mathbf{w}^T\\mathbf{x}}) \\cdot \\mathbf{x} + (1-y)\\cdot \\frac{(1+e^{-\\mathbf{w}^T\\mathbf{x}})}{e^{-\\mathbf{w}^T\\mathbf{x}}}\\cdot \\frac{1}{(1+e^{-\\mathbf{w}^T\\mathbf{x}})^2}\\cdot e^{-\\mathbf{w}^T\\mathbf{x}} \\cdot \\mathbf{x} \\big] = (\\frac{1}{1+e^{-\\mathbf{w}^T\\mathbf{x}}}-y)\\mathbf{x}=(\\hat{y}-y)\\mathbf{x}$$\n",
    "\n",
    "#### [15 Points] Question 2.4: Build a Logistic Regression classifier with the skeleton code below.\n",
    "\n",
    "#### You are allowed to reference the following materials:\n",
    "- Lecture slides and notes from our course.\n",
    "- The [Logistic Regression chapter](https://web.stanford.edu/~jurafsky/slp3/5.pdf) in Dr. Jrafsky and Dr. Martin's *Speech and Language Processing*.\n",
    "- The Wikipedia articles on [Logistic Regression](https://en.wikipedia.org/wiki/Logistic_regression).\n",
    "- Dr. Kilian Q. Weinberger's [course notes](https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote06.html) and lecture recording.\n",
    "- [Scikit-Learn documentation on Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression :\n",
    "\n",
    "    # you will populate self.classes, self.w with the dimensions of the training data in the self.fit() functionj\n",
    "    # losses will be populated during training with the self.fit() function.\n",
    "    def __init__(self) :\n",
    "\n",
    "        self.classes = None\n",
    "        self.w = None\n",
    "        self.losses = []\n",
    "\n",
    "    def sigmoid(self, x) :\n",
    "\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    # This should be your answer from question 2.1\n",
    "    # Adding a small eps value in logs (e.g. np.log(y + eps)) can help with numeric stability.\n",
    "    # you can divide the loss by the number of samples i.e. len(y_true)\n",
    "    def loss(self, y_true, y_pred, eps = 10e-6):\n",
    "\n",
    "        assert y_true.shape == y_pred.shape\n",
    "\n",
    "        return -y_true * np.log(y_pred + eps) - (1 - y_true) * np.log(1 - y_pred + eps)\n",
    "    \n",
    "    def predict_proba(self, x) :\n",
    "        \n",
    "        return self.sigmoid(np.matmul(x, self.w))\n",
    "\n",
    "    def predict(self, x) :\n",
    "        y_pred_probs = self.predict_proba(x)\n",
    "        # one hot encode y_preds\n",
    "        y_preds = np.expand_dims(np.argmax(y_pred_probs, axis = 1), axis = -1)\n",
    "        return np.column_stack([1 - y_preds, y_preds])\n",
    "    \n",
    "    def fit(self, x, y_true, eta = 0.1, C = 0.01, n_iterations = 10000, eps = 10e-6) :\n",
    "\n",
    "        # if you labels is of shape (n_samples,) rather than (n_samples,2), this loop will handle it\n",
    "        if len(y_true.shape) == 1 :\n",
    "            y_true = np.expand_dims(y_true, axis = -1)\n",
    "            y_true = np.column_stack([1-y_true, y_true])\n",
    "\n",
    "        # Number of samples and features\n",
    "        n_features = x.shape[1]\n",
    "        # Get unique class labels\n",
    "        self.classes = np.unique(y_true)\n",
    "        n_classes = len(self.classes)\n",
    "        self.w = np.zeros((n_features, n_classes))\n",
    "\n",
    "        self.losses = []\n",
    "        for _ in range(n_iterations) :\n",
    "\n",
    "            y_pred = self.predict_proba(x)\n",
    "            # the gradient should be your answer from question 2.3\n",
    "            gradient = np.dot(x.T, y_pred - y_true)/len(y_true) + C * self.w\n",
    "            self.losses.append(np.mean(self.loss(y_true, y_pred)))\n",
    "            self.w -= eta * gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [10 Points] Question 3: Training and Evaluating the Classifiers\n",
    "\n",
    "Now that you have done the hard work of building your models, it's time to train!\n",
    "\n",
    "We have already supplied you with starter code to get your `X_train`, `y_test`, etc (don't worry, you'll get plenty of opportunity for data cleaning and processing in later assignments). Instantiate model objects for both `NaiveBayes` and `LogisticRegression` and use their respective `.fit()`, `.predict_proba()`, and `.predict()` methods to train your models and generate predictions on the test set.\n",
    "\n",
    "As we discussed in the preamble, it's important to generate distributional estimates of your performance (rather than point estimates). There are several ways to get these confidence intervals; however, we are going to consider a fairly lightweight method of doing this by resampling the test set to generate our metrics--if you find this task daunting, we suggest starting with the [definition of bootstrapping](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)) (e.g. how do we sample with replacement with `numpy`), what a 95% [confidence interval](https://en.wikipedia.org/wiki/Confidence_interval) is supposed to represent, and [the primer by Dr. Sebastian Rashka](https://sebastianraschka.com/blog/2022/confidence-intervals-for-ml.html) (specifically [Method 3](https://sebastianraschka.com/blog/2022/confidence-intervals-for-ml.html#method-3-bootstrapping-the-test-set-predictions)).\n",
    "\n",
    "#### [10 Points] 3.1: Using the starter code below, train both a `NaiveBayes` and `LogisticRegression` model and generate 95% confidence intervals for Accuracy, AUPRC, and AUROC by bootstrapping the test set. You may use `sklearn.metrics` to generate your scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label counts: train Counter({0: 129, 1: 113}), test Counter({0: 35, 1: 26})\n"
     ]
    }
   ],
   "source": [
    "# STARTER CODE\n",
    "\n",
    "# If you downloaded the data and label files locally, replace the file paths below.\n",
    "DATA_PATH, LABEL_PATH = os.path.join('data', 'heart_disease_data.csv'), os.path.join('data', 'heart_disease_labels.csv')\n",
    "\n",
    "if os.path.isfile(DATA_PATH) and os.path.isfile(LABEL_PATH) :\n",
    "\n",
    "    X = pd.read_csv(DATA_PATH)\n",
    "    y = pd.read_csv(LABEL_PATH)\n",
    "\n",
    "# If you are using the UCI ML Repo package instead:\n",
    "else :\n",
    "\n",
    "    from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "    heart_disease = fetch_ucirepo(id = 45)\n",
    "    X = heart_disease.data.features\n",
    "    y = heart_disease.data.targets\n",
    "\n",
    "# Feature descriptions from the UCI Machine Learning Repository documentation:\n",
    "# cp (categorical): chest pain type (1: typical angina, 2: atypical angina, 3: non-anginal pain, 4: asymptomatic)\n",
    "# trestbps (integer): resting blood pressure (in mm Hg on admission to the hospital)\n",
    "# chol (integer): serum cholestoral in mg/dl\n",
    "# fbs (categorical): (fasting blood sugar > 120 mg/dl)  (1 = true; 0 = false)\n",
    "# restecg (categorical): resting electrocardiographic results (0: normal, 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV), 2: showing probable or definite left ventricular hypertrophy by Estes' criteria)\n",
    "# thalach (integer): maximum heart rate achieved\n",
    "# exang (categorical): exercise induced angina (1 = yes; 0 = no)\n",
    "# oldpeak (integer): ST depression induced by exercise relative to rest\n",
    "# slope (categorical): the slope of the peak exercise ST segment (1: upsloping, 2: flat, 3: downsloping)\n",
    "# ca (integer): number of major vessels (0-3) colored by flourosopy\n",
    "# thal (categorical): 3 = normal; 6 = fixed defect; 7 = reversable defect\n",
    "\n",
    "# Convert labels to binary values.\n",
    "y = y['num'].to_numpy()\n",
    "y[y!=0] = 1\n",
    "\n",
    "data_type_by_columns = {\"age\":\"integer\",\n",
    "                        \"sex\":\"categorical\",\n",
    "                        \"cp\": \"categorical\",\n",
    "                        \"trestbps\": \"integer\",\n",
    "                        \"chol\": \"integer\",\n",
    "                        \"fbs\": \"categorical\",\n",
    "                        \"restecg\": \"categorical\",\n",
    "                        \"thalach\": \"integer\",\n",
    "                        \"exang\": \"categorical\",\n",
    "                        \"oldpeak\": \"integer\",\n",
    "                        \"slope\": \"categorical\",\n",
    "                        \"ca\": \"integer\",\n",
    "                        \"thal\": \"categorical\"}\n",
    "\n",
    "columns_to_convert_to_dummies = [column_name for column_name,data_type in data_type_by_columns.items() if (data_type == 'categorical') and len(set(X[column_name])) > 2]\n",
    "\n",
    "for column_name in columns_to_convert_to_dummies :\n",
    "    current_dummies = pd.get_dummies(X[column_name], prefix = column_name)\n",
    "    X = pd.concat([X, current_dummies], axis = 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "print(f'Label counts: train {Counter(y_train)}, test {Counter(y_test)}')\n",
    "\n",
    "median_imputer = SimpleImputer(missing_values = np.nan, strategy = 'median')\n",
    "median_imputer.fit(X_train)\n",
    "X_train, X_test = median_imputer.transform(X_train), median_imputer.transform(X_test)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train, X_test = scaler.transform(X_train), scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>...</th>\n",
       "      <th>cp_4</th>\n",
       "      <th>restecg_0</th>\n",
       "      <th>restecg_1</th>\n",
       "      <th>restecg_2</th>\n",
       "      <th>slope_1</th>\n",
       "      <th>slope_2</th>\n",
       "      <th>slope_3</th>\n",
       "      <th>thal_3.0</th>\n",
       "      <th>thal_6.0</th>\n",
       "      <th>thal_7.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>160</td>\n",
       "      <td>286</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>108</td>\n",
       "      <td>1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>120</td>\n",
       "      <td>229</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>129</td>\n",
       "      <td>1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>110</td>\n",
       "      <td>264</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>144</td>\n",
       "      <td>193</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>130</td>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>115</td>\n",
       "      <td>1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>138</td>\n",
       "      <td>175</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>303 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  ...  \\\n",
       "0     63    1   1       145   233    1        2      150      0      2.3  ...   \n",
       "1     67    1   4       160   286    0        2      108      1      1.5  ...   \n",
       "2     67    1   4       120   229    0        2      129      1      2.6  ...   \n",
       "3     37    1   3       130   250    0        0      187      0      3.5  ...   \n",
       "4     41    0   2       130   204    0        2      172      0      1.4  ...   \n",
       "..   ...  ...  ..       ...   ...  ...      ...      ...    ...      ...  ...   \n",
       "298   45    1   1       110   264    0        0      132      0      1.2  ...   \n",
       "299   68    1   4       144   193    1        0      141      0      3.4  ...   \n",
       "300   57    1   4       130   131    0        0      115      1      1.2  ...   \n",
       "301   57    0   2       130   236    0        2      174      0      0.0  ...   \n",
       "302   38    1   3       138   175    0        0      173      0      0.0  ...   \n",
       "\n",
       "      cp_4  restecg_0  restecg_1  restecg_2  slope_1  slope_2  slope_3  \\\n",
       "0    False      False      False       True    False    False     True   \n",
       "1     True      False      False       True    False     True    False   \n",
       "2     True      False      False       True    False     True    False   \n",
       "3    False       True      False      False    False    False     True   \n",
       "4    False      False      False       True     True    False    False   \n",
       "..     ...        ...        ...        ...      ...      ...      ...   \n",
       "298  False       True      False      False    False     True    False   \n",
       "299   True       True      False      False    False     True    False   \n",
       "300   True       True      False      False    False     True    False   \n",
       "301  False      False      False       True    False     True    False   \n",
       "302  False       True      False      False     True    False    False   \n",
       "\n",
       "     thal_3.0  thal_6.0  thal_7.0  \n",
       "0       False      True     False  \n",
       "1        True     False     False  \n",
       "2       False     False      True  \n",
       "3        True     False     False  \n",
       "4        True     False     False  \n",
       "..        ...       ...       ...  \n",
       "298     False     False      True  \n",
       "299     False     False      True  \n",
       "300     False     False      True  \n",
       "301      True     False     False  \n",
       "302      True     False     False  \n",
       "\n",
       "[303 rows x 26 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaiveBayes model accuracy: 0.801, 95% CI: [0.689,0.902]\n",
      "NaiveBayes model AUPRC: 0.841, 95% CI: [0.712,0.937]\n",
      "NaiveBayes model AUROC: 0.837, 95% CI: [0.726,0.936]\n"
     ]
    }
   ],
   "source": [
    "NaiveBayes_model = NaiveBayes()\n",
    "NaiveBayes_model.fit(X_train, y_train)\n",
    "y_pred = NaiveBayes_model.predict(X_test)\n",
    "y_pred_proba = NaiveBayes_model.predict_proba(X_test)\n",
    "\n",
    "# generate 95% confidence interval for the accuracy, AUPRC, AUROC of the model\n",
    "n_bootstraps = 1000\n",
    "bootstrap_accuracies = []\n",
    "bootstrap_auprc = []\n",
    "bootstrap_auroc = []\n",
    "for _ in range(n_bootstraps):\n",
    "    indices = np.random.choice(range(len(y_test)), len(y_test), replace=True)\n",
    "    bootstrap_accuracies.append(np.mean(y_test[indices] == y_pred[indices]))\n",
    "    bootstrap_auprc.append(average_precision_score(y_test[indices], y_pred_proba[indices, 1]))\n",
    "    bootstrap_auroc.append(roc_auc_score(y_test[indices], y_pred_proba[indices, 1]))\n",
    "print('NaiveBayes model accuracy: {:.3f}, 95% CI: [{:.3f},{:.3f}]'.format(np.mean(bootstrap_accuracies), np.percentile(bootstrap_accuracies, 2.5), np.percentile(bootstrap_accuracies, 97.5)))\n",
    "print('NaiveBayes model AUPRC: {:.3f}, 95% CI: [{:.3f},{:.3f}]'.format(np.mean(bootstrap_auprc), np.percentile(bootstrap_auprc, 2.5), np.percentile(bootstrap_auprc, 97.5)))\n",
    "print('NaiveBayes model AUROC: {:.3f}, 95% CI: [{:.3f},{:.3f}]'.format(np.mean(bootstrap_auroc), np.percentile(bootstrap_auroc, 2.5), np.percentile(bootstrap_auroc, 97.5)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.786, 95% CI: [0.688,0.885]\n",
      "Logistic Regression AUPRC: 0.839, 95% CI: [0.704,0.940]\n",
      "Logistic Regression AUROC: 0.864, 95% CI: [0.767,0.943]\n"
     ]
    }
   ],
   "source": [
    "# logistic regression\n",
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(X_train, y_train)\n",
    "y_pred = logistic_model.predict(X_test)\n",
    "y_pred_proba = logistic_model.predict_proba(X_test)\n",
    "\n",
    "# generate 95% confidence interval for the accuracy, of the model\n",
    "n_bootstraps = 1000\n",
    "bootstrap_accuracies = []\n",
    "bootstrap_auprc = []\n",
    "bootstrap_auroc = []\n",
    "for _ in range(n_bootstraps):\n",
    "    indices = np.random.choice(range(len(y_test)), len(y_test), replace=True)\n",
    "    bootstrap_accuracies.append(np.mean(y_test[indices] == y_pred[indices,1]))\n",
    "    bootstrap_auprc.append(average_precision_score(y_test[indices], y_pred_proba[indices][:,1]))\n",
    "    bootstrap_auroc.append(roc_auc_score(y_test[indices], y_pred_proba[indices][:,1]))\n",
    "\n",
    "print('Logistic Regression Accuracy: {:.3f}, 95% CI: [{:.3f},{:.3f}]'.format(np.mean(bootstrap_accuracies),np.percentile(bootstrap_accuracies, 2.5), np.percentile(bootstrap_accuracies, 97.5)))\n",
    "print('Logistic Regression AUPRC: {:.3f}, 95% CI: [{:.3f},{:.3f}]'.format(np.mean(bootstrap_auprc), np.percentile(bootstrap_auprc, 2.5), np.percentile(bootstrap_auprc, 97.5)))\n",
    "print('Logistic Regression AUROC: {:.3f}, 95% CI: [{:.3f},{:.3f}]'.format(np.mean(bootstrap_auroc), np.percentile(bootstrap_auroc, 2.5), np.percentile(bootstrap_auroc, 97.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
